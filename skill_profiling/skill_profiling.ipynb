{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6cca42-2b7d-417e-885e-3840ba8e66e2",
   "metadata": {},
   "source": [
    "# Skill profiling\n",
    "\n",
    "This notebook will do some general skill profiling of the analog forecast.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963a5cea-8c29-4d2a-ac19-960cc66bf989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from itertools import product\n",
    "from multiprocessing.pool import Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# import dask\n",
    "# from dask.distributed import Client\n",
    "\n",
    "# local\n",
    "from analog_forecast import make_forecast, spatial_subset, run_rmse_over_time\n",
    "from config import data_dir\n",
    "import luts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178d579-7e21-4ccb-934f-fe820ffe434e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Goal\n",
    "\n",
    "The goal is to compute the error for the analog forecast method and a naive forecast method, for 50 dates. For now, those dates will be randomly chosen, but this workflow may be adapted to accept user supplied dates. The product here should be a table of results - errors between the forecast and \"observed\" values.\n",
    "\n",
    "Some facts:\n",
    "\n",
    "* forecasts will be made for the 14 days post-reference date\n",
    "* forecasts will use 5 analogs\n",
    "* a forecast for a given date is the mean of the corresponding subsequent dates across all analogs\n",
    "* we are not mixing variables or spatial domains or weighting\n",
    "\n",
    "### Processing strategy\n",
    "\n",
    "We have some large data files - daily data for the northern hemisphere for our variables of interest - that will end up being read completely into memory because of the search of analogs over the entire time series. Additionally, the naive forecasting will be sampling many of the time steps. Being ~45GB (or ~23GB for the raw (i.e. non-anomaly-based) files), it will make sense to read the dataset completely into memory and then iterate over the possible groups. So we will iterate over the data files at the lowest level, which are grouped by variable and data type (raw vs anomaly).\n",
    "\n",
    "### Naive profiling\n",
    "\n",
    "I believe we only need to simulate the naive forecasts for each domain and variable, not for every reference date. This assumes that the distribution of \"skill\" (RMSE for now) for the naive forecast is the same for every day of the year. For each spatial domain and variable, we are attempting to simulate the distribution of a naive forecast skill based on selecting uniformly random analogs from the complete historical time series. \n",
    "\n",
    "So, we can create a table of naive forecast skill for all combinations of spatial domain and variable, which can then be joined with a table of analog forecast results for useful comparisons. Define some functions to do this that can be used to iterate over all combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "569bd64b-17a8-4c24-affb-20d39a219f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_error(sub_da, times, ref_date):\n",
    "    forecast = make_forecast(sub_da, times, ref_date)\n",
    "    err = sub_da.sel(time=forecast.time.values) - forecast\n",
    "    return (err ** 2).mean(axis=(1, 2)).drop(\"time\")\n",
    "\n",
    "\n",
    "def _find_analogs(da, ref_date):\n",
    "    \"\"\"Differs from module implementation of same name to operate on already loaded dataset\"\"\"\n",
    "    # compute RMSE between ref_date and all preceding dates \n",
    "    #  for the specified variable and spatial domain\n",
    "    rmse_da = run_rmse_over_time(da, ref_date)\n",
    "    # sort before dropping duplicated years\n",
    "    rmse_da = rmse_da.sortby(rmse_da)\n",
    "    # drop duplicated years.\n",
    "    # This is being done because analogs might occur in the same year as the\n",
    "    #  reference date and notes from meetings with collaborators indicate that\n",
    "    #  there should only be one analog per year, as was the case for the\n",
    "    #  previous iteration of the algorithm.\n",
    "    keep_indices = ~pd.Series(rmse_da.time.dt.year).duplicated()\n",
    "    analogs = rmse_da.isel(time=keep_indices)\n",
    "    # subset to first 5 analogs for now\n",
    "    analogs = analogs.isel(time=slice(5))\n",
    "\n",
    "    return analogs\n",
    "\n",
    "\n",
    "def run_analog_forecast(da, ref_date):\n",
    "    analogs = _find_analogs(da, ref_date)\n",
    "    analog_error = forecast_and_error(da, analogs.time.values, ref_date)\n",
    "    \n",
    "    # start a table entry\n",
    "    err_df = pd.DataFrame({\n",
    "        \"reference_date\": ref_date,\n",
    "        \"forecast_day_number\": np.arange(14) + 1,\n",
    "        \"forecast_error\": analog_error.values,\n",
    "    })\n",
    "    \n",
    "    return err_df\n",
    "\n",
    "\n",
    "def profile_analog_forecast(da, ref_dates, ncpus=2):\n",
    "    \"\"\"Profile the analog forecast for a given data array at the different ref_dates.\n",
    "    Return a dataframe of results.\n",
    "    \"\"\"\n",
    "    results = [run_analog_forecast(sub_da, date) for date in ref_dates]\n",
    "    err_df = pd.concat(results)\n",
    "    \n",
    "    # these attributes are constant for all reference dates\n",
    "    err_df[\"variable\"] = varname\n",
    "    err_df[\"spatial_domain\"] = spatial_domain\n",
    "    err_df[\"anomaly_search\"] = use_anom\n",
    "    # reorder columns\n",
    "    err_df = err_df[[\n",
    "        \"variable\", \n",
    "        \"spatial_domain\", \n",
    "        \"anomaly_search\", \n",
    "        \"reference_date\", \n",
    "        \"forecast_day_number\", \n",
    "        \"forecast_error\"\n",
    "    ]]\n",
    "    \n",
    "    return err_df\n",
    "\n",
    "\n",
    "def get_naive_sample_dates(all_times, naive_ref_date):\n",
    "    \"\"\"Constructs list of all dates to be queried in some fashion for an instance of the naive forecast\"\"\"\n",
    "    analog_times = list(np.random.choice(all_times, 5, replace=False))\n",
    "    naive_ref_date = pd.to_datetime(naive_ref_date + \" 12:00:00\").to_numpy()\n",
    "    # if any sample times are closer than 15 days, re-sample\n",
    "    while np.any((np.diff(sorted(analog_times + [naive_ref_date])) / (10**9 * 60 * 60 * 24)).astype(int) <= 14):\n",
    "        analog_times = list(np.random.choice(all_times, 5, replace=False))\n",
    "    \n",
    "    all_dates = []\n",
    "    for t in analog_times + [naive_ref_date]:\n",
    "        all_dates.extend(pd.date_range(t, t + pd.to_timedelta(14, unit=\"d\")))\n",
    "    \n",
    "    return all_dates, analog_times\n",
    "\n",
    "\n",
    "def profile_naive_forecast(da, n=1000, ncpus=16):\n",
    "    \"\"\"Profiles the naive forecast method using a single data array with time, latitude, and longitude dimensions.\n",
    "    Return a dataframe of results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        # significant speed-up in pooling achieved by first sub-selecting the times of interest from in-memory datarray\n",
    "        #  times of interest will be the naive analog dates, the reference date, and the 14 days after all of them.\n",
    "        # (not sure if the above really applies with non-Pool-based method now, but it shouldn't hurt)\n",
    "        all_naive_dates, naive_analog_dates = get_naive_sample_dates(sub_da.time.values[:-15], naive_ref_date)\n",
    "        results.append(forecast_and_error(sub_da.sel(time=all_naive_dates), naive_analog_dates, naive_ref_date))\n",
    "    \n",
    "    sim_rmse = xr.concat(results, pd.Index(range(n), name=\"sim\"))\n",
    "\n",
    "    err_df = pd.DataFrame({\n",
    "        \"variable\": da.name,\n",
    "        \"spatial_domain\": spatial_domain,\n",
    "        \"anomaly_search\": use_anom,\n",
    "        \"forecast_day_number\": np.arange(14) + 1,\n",
    "        \"naive_2.5\": sim_rmse.reduce(np.percentile, dim=\"sim\", q=2.5),\n",
    "        \"naive_50\": sim_rmse.reduce(np.percentile, dim=\"sim\", q=50),\n",
    "        \"naive_97.5\": sim_rmse.reduce(np.percentile, dim=\"sim\", q=97.5),\n",
    "    })\n",
    "    \n",
    "    return err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4175a03-e775-4329-ba85-55a5ab57696b",
   "metadata": {},
   "source": [
    "Set the reference dates for the analog forecasting, and take the first one to be the reference date for the naive forecast simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794ce214-ceca-44f6-872b-b6afbb1d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open arbitrary dataset to get a constant set of reference dates for analog profiling\n",
    "with xr.open_dataset(data_dir.joinpath(\"era5_2m_temperature_hour12_1959_2021.nc\")) as ds:\n",
    "    np.random.seed(907)\n",
    "    ref_dates = np.random.choice(ds.time.values[:-15], 10, replace=False)\n",
    "    ref_dates = [pd.to_datetime(date).strftime(\"%Y-%m-%d\") for date in ref_dates]\n",
    "    \n",
    "# arbitrary reference date for naive forecasts\n",
    "naive_ref_date = ref_dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ddcc7-085b-4683-ae32-b61a55c86ba4",
   "metadata": {},
   "source": [
    "Iterate over the variable/data type combinations, load the data, then iterate over spatial domains for both forecasts, iterating over reference dates only for the analog forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c79425e-d8e1-4bb4-88d4-d2f65f1f13c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/atlas_scratch/kmredilla/analog_forecast/era5_2m_temperature_anom_1959_2021.nc done, 20m elapsed\n",
      "/atlas_scratch/kmredilla/analog_forecast/era5_2m_temperature_hour12_1959_2021.nc done, 43m elapsed\n",
      "/atlas_scratch/kmredilla/analog_forecast/era5_mean_sea_level_pressure_anom_1959_2021.nc done, 63m elapsed\n",
      "/atlas_scratch/kmredilla/analog_forecast/era5_mean_sea_level_pressure_hour12_1959_2021.nc done, 91m elapsed\n",
      "/atlas_scratch/kmredilla/analog_forecast/era5_sea_surface_temperature_anom_1959_2021.nc done, 114m elapsed\n",
      "/atlas_scratch/kmredilla/analog_forecast/era5_sea_surface_temperature_hour12_1959_2021.nc done, 140m elapsed\n",
      "/atlas_scratch/kmredilla/analog_forecast/era5_geopotential_anom_1959_2021.nc done, 162m elapsed\n",
      "/atlas_scratch/kmredilla/analog_forecast/era5_geopotential_hour12_1959_2021.nc done, 187m elapsed\n",
      "CPU times: user 1h 31min 34s, sys: 1h 34min 17s, total: 3h 5min 51s\n",
      "Wall time: 3h 7min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# accumulators for results\n",
    "naive_results = []\n",
    "analog_results = []\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for varname, use_anom in product(luts.varnames_lu.keys(), [True, False]):\n",
    "    fp_lu_key = {True: \"anom_filename\", False: \"filename\"}[use_anom]\n",
    "    fp = data_dir.joinpath(luts.varnames_lu[varname][fp_lu_key])\n",
    "    ds = xr.load_dataset(fp)\n",
    "    \n",
    "    for spatial_domain in luts.spatial_domains:\n",
    "        bbox = luts.spatial_domains[spatial_domain][\"bbox\"]\n",
    "        sub_da = spatial_subset(ds[varname], bbox)\n",
    "        \n",
    "        # profile the analog forecast by computing for all dates\n",
    "        analog_results.append(profile_analog_forecast(sub_da, ref_dates))\n",
    "        # profile the naive forecast\n",
    "        naive_results.append(profile_naive_forecast(sub_da))\n",
    "    print(f\"{fp} done, {round((time.perf_counter() - tic) / 60)}m elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca06683e-7ab8-4e59-abea-a6bd3dabd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_df = pd.concat(naive_results)\n",
    "analog_df = pd.concat(analog_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac767990-06b9-4268-a4c4-9dd59c43b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analog_df.round(3).to_csv(\"analog_profiling_results.csv\", index=False)\n",
    "naive_df.round(3).to_csv(\"naive_profiling_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ec8db-c2a3-40f4-bc2e-331f38b58386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
