{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6cca42-2b7d-417e-885e-3840ba8e66e2",
   "metadata": {},
   "source": [
    "# Skill profiling\n",
    "\n",
    "This notebook will orchestrate a skill profiling of the analog forecast algorithm across all available options for a set of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62333829-afdb-4bd3-b5f8-010f409e6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import luts\n",
    "from config import data_dir, project_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178d579-7e21-4ccb-934f-fe820ffe434e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Goal\n",
    "\n",
    "The goal is to compute the error for the analog forecast method and a naive forecast method. The product here should be a table of results - errors between the forecast and \"observed\" values for all spatial domains, variables, etc.\n",
    "\n",
    "### Processing strategy\n",
    "\n",
    "We have some large data files - daily data for the northern hemisphere for our variables of interest - that will end up being read completely into memory because of the search of analogs over the entire time series for that full domain as well as subdomains. Additionally, the naive forecasting will be sampling many of the time steps over many simulations. Being ~45GB (or ~23GB for the raw (i.e. non-anomaly-based) files), it will make sense to read the dataset completely into memory and then iterate over the possible groups. So we will iterate over the data files at the lowest level, which are grouped by variable and data type (raw vs anomaly) for 8 files.\n",
    "\n",
    "We will use slurm here to execute the `run_profile.py` script, which will conduct the profiling for all dates specified in that file. \n",
    "\n",
    "### Naive profiling\n",
    "\n",
    "I believe we only need to simulate the naive forecasts for each domain and variable, not for every reference date. This assumes that the distribution of \"skill\" (RMSE for now) for the naive forecast is the same for every day of the year. For each spatial domain and variable, we are attempting to simulate the distribution of a naive forecast skill based on selecting uniformly random analogs from the complete historical time series. \n",
    "\n",
    "So, we can create a table of naive forecast skill for all combinations of spatial domain and variable, which can then be joined with a table of analog forecast results for useful comparisons. \n",
    "\n",
    "We will use slurm for this because each group takes a while to process. We will call the `run_profile.py` script to the profiling for a particular variable and data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e83f3ac-9fee-499b-856a-da96d35b6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_run_profile_sbatch(\n",
    "    sbatch_fp, \n",
    "    sbatch_out_fp, \n",
    "    varname, \n",
    "    results_fp, \n",
    "    use_anom, \n",
    "    data_dir, \n",
    "    project_dir, \n",
    "    conda_init_script\n",
    "):\n",
    "    sbatch_head = (\n",
    "        \"#!/bin/sh\\n\"\n",
    "        \"#SBATCH --nodes=1\\n\"\n",
    "        \"#SBATCH --cpus-per-task=32\\n\"\n",
    "        \"#SBATCH --exclusive\\n\"\n",
    "        \"#SBATCH --mail-type=FAIL\\n\"\n",
    "        f\"#SBATCH --mail-user=kmredilla@alaska.edu\\n\"\n",
    "        f\"#SBATCH -p main\\n\"\n",
    "        f\"#SBATCH --output {sbatch_out_fp}\\n\"\n",
    "        f\"source {conda_init_script}\\n\"\n",
    "        \"conda activate analog-forecast\\n\"\n",
    "        f\"export DATA_DIR={data_dir}\\n\"\n",
    "        f\"export PYTHONPATH=$PYTHONPATH:{project_dir}\\n\"\n",
    "    )\n",
    "\n",
    "    py_commands = (\n",
    "        f\"time python {project_dir.joinpath('skill_profiling', 'run_profile.py')} \"\n",
    "        f\"--varname {varname} \"\n",
    "        f\"--results_file {results_fp} \"\n",
    "        f\"{'--use_anom' if use_anom else ''} \"\n",
    "    )\n",
    "\n",
    "    commands = sbatch_head + py_commands\n",
    "\n",
    "    with open(sbatch_fp, \"w\") as f:\n",
    "        f.write(commands)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488c419-79c6-49de-b2ba-92ce1d5e678e",
   "metadata": {},
   "source": [
    "Make the slurm scripts for `sbatch`ing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6ea44b-35bb-4e90-9eea-ff034ca2039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch_dir = Path(\"slurm\")\n",
    "sbatch_dir.mkdir(exist_ok=True)\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "sbatch_fps = []\n",
    "results_fps = []\n",
    "\n",
    "# get the path to the conda init script\n",
    "conda_init_script = os.getenv(\"CONDA_INIT_SCRIPT\")\n",
    "\n",
    "for varname in luts.varnames_lu.keys():\n",
    "    for use_anom in [True, False]:\n",
    "        group_str = f\"{varname}{'_anom' if use_anom else ''}\"\n",
    "        sbatch_fp = sbatch_dir.joinpath(f\"run_profile_{group_str}.slurm\").resolve()\n",
    "        sbatch_out_fp = sbatch_dir.joinpath(f\"run_profile_{group_str}_%j.out\").resolve()\n",
    "        results_fp = results_dir.joinpath(f\"{group_str}.csv\").resolve()\n",
    "        sbatch_kwargs = {\n",
    "            \"sbatch_fp\": sbatch_fp,\n",
    "            \"sbatch_out_fp\": sbatch_out_fp,\n",
    "            \"varname\": varname,\n",
    "            \"results_fp\": results_fp,\n",
    "            \"use_anom\": use_anom,\n",
    "            \"data_dir\": data_dir,\n",
    "            \"project_dir\": project_dir,\n",
    "            \"conda_init_script\": conda_init_script\n",
    "        }\n",
    "        \n",
    "        write_run_profile_sbatch(**sbatch_kwargs)\n",
    "        sbatch_fps.append(sbatch_fp)\n",
    "        results_fps.append(results_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6334c-c87b-4626-8bc6-86ab4fc1b311",
   "metadata": {},
   "source": [
    "Submit the jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0fc5ad7-2f60-4044-82c7-19f114796fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [subprocess.check_output([\"sbatch\", str(fp)]) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b514580-c2da-4c15-a0f6-80206d433011",
   "metadata": {},
   "source": [
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
